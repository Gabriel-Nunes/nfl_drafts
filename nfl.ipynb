{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from random import choice\n",
    "from selenium import webdriver\n",
    "import pandas as pd\n",
    "from selenium.webdriver.firefox.options import Options\n",
    "from selenium.webdriver.common.by import By\n",
    "from selenium.webdriver.support import expected_conditions as EC\n",
    "from selenium.webdriver.support.ui import WebDriverWait\n",
    "import geckodriver_autoinstaller\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "from time import sleep\n",
    "from random import uniform\n",
    "import re\n",
    "\n",
    "geckodriver_autoinstaller.install()\n",
    "\n",
    "class Browser():\n",
    "    def __init__(self) -> None:\n",
    "        \"\"\"\n",
    "        Start a new browser and get it ready to crawl.\n",
    "        \"\"\"\n",
    "\n",
    "        self.options = Options()\n",
    "        self.options.add_argument(\"--start-maximized\")\n",
    "        self.options.add_argument(\"--private\")\n",
    "        self.options.add_argument('--no-sandbox')\n",
    "        self.options.add_argument(\"--ssl-protocol=any\")\n",
    "        self.options.add_argument(\"--ignore-certificate-errors\")\n",
    "\n",
    "        # Set user-agent\n",
    "        # with open(os.path.join('src', 'user_agents.txt')) as file:\n",
    "        #     user_agents = [ua.strip() for ua in file.readlines()]\n",
    "        #     user_agent = choice(user_agents)\n",
    "        #     # print(user_agent)        \n",
    "        # self.options.add_argument('user-agent={0}'.format(user_agent))    \n",
    "                \n",
    "        # self.options.headless = True\n",
    "        \n",
    "        # Create a webdriver instance\n",
    "        self.driver = webdriver.Firefox(\n",
    "            firefox_binary='/home/gabriel/Downloads/firefox/firefox',\n",
    "            options=self.options,\n",
    "\n",
    "            )\n",
    "        self.driver.maximize_window()\n",
    "\n",
    "    def go(self, url, end_of_page, *args, **kwargs):\n",
    "        self.driver.get(url)\n",
    "        # Wait until the end of the page load\n",
    "        WebDriverWait(self.driver, 200).until(EC.presence_of_element_located((By.XPATH, end_of_page)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_109950/863404624.py:41: DeprecationWarning: firefox_binary has been deprecated, please pass in a Service object\n",
      "  self.driver = webdriver.Firefox(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2020\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 506/506 [45:20<00:00,  5.38s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2019\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 503/503 [2:01:39<00:00, 14.51s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2018\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 642/642 [2:34:08<00:00, 14.41s/it]  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2017\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 424/424 [1:42:29<00:00, 14.50s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2016\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 407/407 [1:38:26<00:00, 14.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2015\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 408/408 [1:38:33<00:00, 14.49s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Extracting 2014\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 462/462 [1:51:58<00:00, 14.54s/it]\n"
     ]
    }
   ],
   "source": [
    "browser = Browser()\n",
    "for draft_year in [2020, 2019, 2018, 2017, 2016, 2015, 2014]:\n",
    "    print()\n",
    "    print(f'Extracting {draft_year}')\n",
    "    \n",
    "    results = []\n",
    "\n",
    "    def get_players_db():\n",
    "        # Get a DataFrame with extracted players json files\n",
    "        players_list = []\n",
    "        for root, folders, files in os.walk('data'):\n",
    "            for file in files:\n",
    "                with open(os.path.join(root, file)) as f:\n",
    "                    data = json.load(f)\n",
    "                    players_list.append(data)\n",
    "        players = pd.DataFrame(players_list)\n",
    "        return players\n",
    "\n",
    "    def get_links():\n",
    "\n",
    "        # Get num of results pages\n",
    "        browser.go(f'https://www.nfl.com/draft/tracker/prospects/all-positions/all-colleges/all-statuses/{draft_year}?page=1',\"//button[@id='ot-sdk-btn']\")\n",
    "        sleep(7)\n",
    "        pagination = browser.driver.find_element(By.XPATH, '//div[contains(text(), \"Page \")]').text\n",
    "        num_pages = int(re.search('\\D(\\d+)$', pagination).group(1))\n",
    "\n",
    "        # Get the links of draft_year\n",
    "        links = []\n",
    "        for i in range(1, num_pages + 1):\n",
    "            browser.go(f'https://www.nfl.com/draft/tracker/prospects/all-positions/all-colleges/all-statuses/{draft_year}?page={i}',\"//button[@id='ot-sdk-btn']\")\n",
    "            sleep(uniform(3.8, 5.9))\n",
    "            names_elements = browser.driver.find_elements(By.XPATH, '//div[@data-test-id=\"facemask-simple-tile-headerText\"]')\n",
    "            for element in names_elements:\n",
    "                link = element.find_element(By.XPATH, './/a').get_attribute('href')\n",
    "                links.append(link)\n",
    "        return links\n",
    "\n",
    "    players = get_players_db()\n",
    "    links = get_links()\n",
    "\n",
    "    for link in tqdm(links):\n",
    "        # Extract player data from link and save in a json data file\n",
    "        if link not in players['url'].tolist():\n",
    "            sleep(uniform(3.8, 5.9))\n",
    "            browser.go(link, '//*[@id=\"ot-sdk-btn\"]')\n",
    "            sleep(8)\n",
    "            name = browser.driver.find_element(By.XPATH, '//div[@data-testid=\"prospectHeadshotCard\"]//h2').text\n",
    "            college = browser.driver.find_element(By.XPATH, '//div[@data-testid=\"prospectInfoCard\"]//div[3]').text\n",
    "            hometown = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[5]/div[1]/div').text \n",
    "            className = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[1]/div/div/div/div/div[2]/div/div/div/div/div/div[5]/div[2]/div').text\n",
    "            grade = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[1]/div/div/div/div/div[3]/div/div/div/div/div[1]/div[1]').text\n",
    "            try:\n",
    "                drafted_by = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[1]/div/div/div/div/div[4]/div/div/div/div/div[1]/img').get_attribute('alt')\n",
    "            except:\n",
    "                drafted_by = ''\n",
    "            try:\n",
    "                round_elem = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[1]/div/div/div/div/div[4]/div/div/div/div/div[1]/div[4]').text\n",
    "                round = re.sub(r'Round\\s(\\d+).*?Pick\\s+.*', r'\\1', round_elem)\n",
    "                pick = re.sub(r'Round\\s\\d+.*?Pick\\s+(.*)', r'\\1', round_elem)\n",
    "            except:\n",
    "                try:\n",
    "                    round_elem = browser.driver.find_element(By.XPATH, '//div[contains(text(), \"Draft Projection\")]/following-sibling::div').text\n",
    "                    round = re.sub(r'Round\\s+(\\d+)', r'\\1', round_elem)\n",
    "                    pick = ''\n",
    "                except:\n",
    "                    round = ''\n",
    "                    pick = ''\n",
    "            production_score = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[2]/div/div/div/div/div/div/div[1]/div[2]').text\n",
    "            athleticism_score = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[2]/div/div/div/div/div/div/div[2]/div[2]').text\n",
    "            total_score = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[2]/div/div/div/div/div/div/div[3]/div[2]').text\n",
    "            try:\n",
    "                bio = browser.driver.find_element(By.XPATH, '//div[contains(text(), \"Player Bio\")]/parent::*/following-sibling::div').text\n",
    "            except:\n",
    "                bio = ''\n",
    "            try:\n",
    "                overview = browser.driver.find_element(By.XPATH, '//div[contains(text(), \"Overview\")]/parent::div').get_attribute('innerText')\n",
    "                overview = re.sub(r'Overview|\\n|', '', overview).replace(\"\\'\", '')\n",
    "            except:\n",
    "                overview = ''\n",
    "            strengths_list = browser.driver.find_elements(By.XPATH, '//div[contains(text(), \"Strengths\")]/following-sibling::ul/li')\n",
    "            strengths = ' '.join([e.text for e in strengths_list])\n",
    "            weaknesses_list = browser.driver.find_elements(By.XPATH, '//div[contains(text(), \"Weaknesses\")]/following-sibling::ul/li')\n",
    "            weaknesses = ' '.join([e.text for e in weaknesses_list])\n",
    "            try:\n",
    "                analysis_by = browser.driver.find_element(By.XPATH, '//*[@id=\"main-content\"]/div/div/div/div/div[6]/div/div/div/div/div/div[1]/div/div[1]/div[1]').text\n",
    "            except:\n",
    "                try:\n",
    "                    analysis_by = browser.driver.find_element(By.XPATH, '//div[contains(text(), \"Analysis\")]/parent::button/following-sibling::div/div/div/div/div').text\n",
    "                except:\n",
    "                    analysis_by = ''\n",
    "            year = draft_year\n",
    "\n",
    "            data = {\n",
    "                'url': link,\n",
    "                'name': name,\n",
    "                'college': college,\n",
    "                'hometown': hometown,\n",
    "                'class': className,\n",
    "                'grade': grade,\n",
    "                'drafted_by': drafted_by,\n",
    "                'round': round,\n",
    "                'pick': pick,\n",
    "                'production_score': production_score,\n",
    "                'athleticism_score': athleticism_score,\n",
    "                'total_score': total_score,\n",
    "                'bio': bio,\n",
    "                'overview': overview,\n",
    "                'strengths': strengths,\n",
    "                'weaknesses': weaknesses,\n",
    "                'analysis_by': analysis_by,\n",
    "                'year': year,\n",
    "            }\n",
    "\n",
    "            results.append(data)\n",
    "\n",
    "            with open(f'data/{name}.json', 'w') as file:\n",
    "                data_json = json.dump(data, file)\n",
    "\n",
    "    df = pd.DataFrame(results)\n",
    "    df.to_excel(f'drafts_{draft_year}.xlsx', index=False)\n",
    "\n",
    "final = []\n",
    "# Save all json data files in an excel file\n",
    "for root, folders, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file)) as data_file:\n",
    "            data = json.load(data_file)\n",
    "            final.append(data)\n",
    "\n",
    "final_df = pd.DataFrame(final)\n",
    "final_df.to_excel('final.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "results = []\n",
    "for root, folders, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file)) as data_file:\n",
    "            data = json.load(data_file)\n",
    "            results.append(data)\n",
    "\n",
    "df = pd.DataFrame(results)\n",
    "df.to_excel(f'drafts_{draft_year}.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "browser.driver.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_players = []\n",
    "for root, folders, files in os.walk('data'):\n",
    "    for file in files:\n",
    "        with open(os.path.join(root, file)) as f:\n",
    "            data = json.load(f)\n",
    "            all_players.append(data)\n",
    "final_df = pd.DataFrame(all_players)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021 = final_df.loc[final_df['year'] == 2021]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_2021.to_excel('drafts_2021.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2019    967\n",
       "2018    636\n",
       "2020    500\n",
       "2021    463\n",
       "2014    462\n",
       "2017    421\n",
       "2016    407\n",
       "2015    406\n",
       "2022      1\n",
       "Name: year, dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "final_df['year'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drafts = pd.read_excel('drafts_2014_2022.xlsx')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drafts.to_excel('all_drafts.xlsx', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_drafts['url'] = all_drafts['url'].apply(lambda x: x.strip())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5af60617f4538d9c14f34cb5188fcfdb9f3a34b764b61d7bb3dbed5f542a9185"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
